---
title: '02: Create an SAP account'
layout: default
nav_order: 2
parent: 'Exercise 02: Create and configure resources'
---

# Task 02: Create a data pipeline

## Introduction

Microsoft Fabric provides many methods for ingesting data. You can create data flows, copy jobs, and pipelines. The choice of which method to use is based on your needs. For example, if you need to transform data during ingestion, data flows are your best bet. For this lab, you'll use a data pipeline. The pipeline allows you to easily ingest data at scale.

## Description

In this task, you'll create a pipeline activity within the workspace that uses the Copy data assistant to configure a connection to the SAP Gateway Demo system data store. The Copy Data assistant also creates a lakehouse as a destination for the copied data. You'll use an OData connector to establish a connection with the SAP Gateway demo system.

## Success criteria

-   The data pipeline run successfully completes.

## Learning resources

-   [Create your first pipeline](https://learn.microsoft.com/en-us/fabric/data-factory/create-first-pipeline-with-sample-data "Create your first pipeline")
-   [Move and transform data with dataflows and data pipelines](https://learn.microsoft.com/en-us/fabric/data-factory/transform-data "Move and transform data with dataflows and data pipelines")
-   [OData Overview](https://learn.microsoft.com/en-us/odata/overview "OData Overview")

## Key tasks

### 01: Initialize the pipeline

<details markdown="block"> 
  <summary><strong>Expand this section to view the solution</strong></summary>

1. At the upper left of the SapWS@lab.LabInstance.Id workspace page, select **+ New item**.

    ![3qggsmws.jpg](../../media/3qggsmws.jpg)

1. In the **New item** pane, move through the list to the **Get data** section and select the **Data pipeline** tile.

    ![aubmzchb.jpg](../../media/aubmzchb.jpg)

1. In the **New pipeline** dialog that displays, enter `SapDataPipeline@lab.LabInstance.Id` and then select **Create**. 

    ![8enugciz.jpg](../../media/8enugciz.jpg)

1. On the **Build a data pipeline...** page, select **Copy data assistant**.

    {: .note }
	> The Copy Data assistant steps you through the process of connecting to source data, selecting the data that you want to ingest, selecting your destination, and then running the pipeline.

    ![f8lqvaox.jpg](../../media/f8lqvaox.jpg)

1. On the **Choose data source** page of the **Copy data** tool, enter `OData` in the Search field and select the **OData** tile in the list of results.

    {: .note }
	> Fabric provides a wide variety of connectors so that you can access data from many sources.

    ![nqhubzd1.jpg](../../media/nqhubzd1.jpg)

1. On the **Connect to a data source** page of the **Copy data** tool, enter the values from the following table and then select **Next**. Leave all other options at default settings.

    | Field | Value |
    |:---------|:--------  |
    | URL   | `https://sapes5.sapdevcenter.com/sap/opu/odata/iwbep/GWSAMPLE_BASIC`   |
    | Authentication kind   | **Basic**   |
    | Username   | `the SAP Gateway Demo system user ID`    |
    | Password   | `your SAP portal password` |

    ![odsx5jhd.jpg](../../media/odsx5jhd.jpg)

    {: .warning }
	> It may take several minutes for the tool to connect to the data source and retrieve a list of tables.

    {: .note }
	> The URL provides the path to the GWSAMPLE_BASIC dataset that you'll use for this lab. You generated the user name in Exercise 01, Task 02 when you set up an SAP devcenter account. The SAP Gateway Demo system provides 16 tables for the connection that you're using in this lab. You'll only use six tables for this lab.

1.  On the **Connect to a data source** page of the **Copy data** tool, select the following six tables and then select **Next**:

    - **BusinessPartnerSet**
    - **ContactSet**
    - **ProductSet**
    - **SalesOrderLineItemSet**
    - **SalesOrderSet**
    - **VH_CategorySet**


    ![zceq0oge.jpg](../../media/zceq0oge.jpg)

1. On the **Choose data destination** page of the **Copy data** tool, select the **Lakehouse** tile.

    ![s34z51gi.jpg](../../media/s34z51gi.jpg)

1. In the **New lakehouse** dialog, enter `SapLH@lab.LabInstance.Id` in the **Name** field and then select **Create and connect**.

    ![zt8s89i9.jpg](../../media/zt8s89i9.jpg)

</details>

### 02: Map, save, and run

<details markdown="block"> 
  <summary><strong>Expand this section to view the solution</strong></summary>

1. On the **Connect to data destination** page of the **Copy data** tool, select **BusinessPartnerSet**. Verify that the value for the **Load settings** field is **Load to new table**. Repeat this process for the other five columns. 

    {: .warning }
	> When the **Connect to data destination** page first displays, table metadata is still being loaded into memory. If you see a message stating that tables are loading please wait a few moments for the process to complete and then repeat the step.
        
    ![02igzx2g.jpg](../../media/02igzx2g.jpg)

1. On the **Connect to data destination** page of the **Copy data** tool, select **Next**.

    ![nrgj9ie4.jpg](../../media/nrgj9ie4.jpg)

    {: .warning }
	> If you see error messages stating that tables are not loaded or that column mapping is required, wait a few moments and repeat Step 7. The tool should automatically load all tables and correctly map all columns. 
    >
	> ![f2uh7p7c.jpg](../../media/f2uh7p7c.jpg)

1. On the **Connect to data destination** page of the **Copy data** tool, select **Save + Run**. 

    ![r4bs8kkc.jpg](../../media/r4bs8kkc.jpg)

1. The **SapDataPipeline@lab.LabInstance.Id** page displays.

    {: .note }
	> The tool creates a ForEach activity that cycles through each of the data source tables. For each table, the tool runs a **Copy** activity to fetch the data for the table. The tool automatcially names activities and objects.

    ![gg64issl.jpg](../../media/gg64issl.jpg)

1. In the pipeline run pane that displays, select **OK**. This tells the tool to start the pipeline execution.

    ![3ulcan21.jpg](../../media/3ulcan21.jpg)

    {: .note }
	> During testing, the average time to load data for all six tables was 3-5 minutes. At the bottom of the SapDataPipeline@lab.LabInstance.Id page, you'll see a list of the activities. An **Activity Status** field shows the progress for each activity. The status for each activity changes from **Queued** to **In progress** and finally to **Succeeded**.

1. If necessary, drag the horizontal splitter bar up so that you can see the **Details** pane that appears at the bottom of the page.

    ![lmbirtzs.jpg](../../media/lmbirtzs.jpg)

1. At the bottom of the **SapDataPipeline@lab.LabInstance.Id** page, verify that the value for each Activity status field is **Succeeded**.

    ![53ilm4jc.jpg](../../media/53ilm4jc.jpg)

</details>